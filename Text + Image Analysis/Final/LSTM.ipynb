{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "import pandas as pd\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "import json\n",
    "import gensim\n",
    "import glob\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import re\n",
    "from keras import utils\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Google's pre-trained Word2Vec model.\n",
    "from gensim.models import KeyedVectors\n",
    "word_vectors = KeyedVectors.load_word2vec_format('google_w2vec.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print Word2Vec Specifications\n",
    "vecsize = word_vectors.vector_size\n",
    "print('Vocabulary Size:', len(word_vectors.vocab))\n",
    "print(type(word_vectors.vocab))\n",
    "print('Vector Size:', vecsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define File-Path to Users Folder\n",
    "folder_list = ['#foodporn',\"#nightlife\",\"#cosmetics\",\"#rockclimbing\"]\n",
    "image_path = \"/Users/kmotwani/Desktop/Me/Education/Courses/Capstone Project/Insta Images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Function to get images from path\n",
    "def get_df(path, list_input, thresh):\n",
    "    final_list = []\n",
    "    for ind, i in enumerate(list_input):\n",
    "        temp_path = path + i\n",
    "        print(i,\"\\n\")\n",
    "        count = 0\n",
    "        for j in glob.glob(temp_path + '/*.jpg'):\n",
    "            temp_dict = {}\n",
    "            file_name = j.replace(temp_path,'')[1:]\n",
    "            img = image.load_img(j, target_size=(128, 128))\n",
    "            try:\n",
    "                with open(temp_path+\"/\"+file_name[:-4]+'.txt', encoding=\"utf-8\") as f:\n",
    "                    content = f.readlines()\n",
    "                    caption = ' '.join([x.strip() for x in content])\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "            temp_dict['File'], temp_dict['Response'] = file_name, int(ind)\n",
    "            temp_dict['Image'], temp_dict['Caption'] = np.array(img), caption\n",
    "            final_list.append(temp_dict)\n",
    "            count += 1\n",
    "            if count==thresh:\n",
    "                break\n",
    "    return pd.DataFrame(final_list) \n",
    "\n",
    "\n",
    "#Get Images from User List and Path\n",
    "df = get_df(image_path, folder_list, 9000)\n",
    "print(\"Number of images loaded:\", len(df))\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Hashtags\n",
    "df['Caption'] = df['Caption'].str.replace('#foodporn','', case=False)\n",
    "df['Caption'] = df['Caption'].str.replace('#rockclimbing','', case=False)\n",
    "df['Caption'] = df['Caption'].str.replace('#nightlife','', case=False)\n",
    "df['Caption'] = df['Caption'].str.replace('#cosmetics','', case=False)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into Train and Test set\n",
    "use_df = df.sample(frac=1).reset_index(drop=True)\n",
    "display(use_df.head())\n",
    "np.random.seed(9001)\n",
    "msk = np.random.rand(len(use_df)) < 0.7\n",
    "total_data_train = use_df[msk]\n",
    "total_data_test = use_df[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to get text vector\n",
    "def get_vector(x, limit):\n",
    "    sequence, count = np.zeros((limit, 300), dtype=float), 0\n",
    "    x = re.sub(r'[^\\w\\s]','',x)\n",
    "    for word in x.split():\n",
    "        if word in word_vectors.vocab:\n",
    "            if count<limit: \n",
    "                sequence[count] = word_vectors.get_vector(word)\n",
    "        count += 1\n",
    "    return sequence\n",
    "\n",
    "#Define Train and Test Dataframes\n",
    "x_train, x_test = [], []\n",
    "total_data_train['Caption'].astype(str).apply(lambda x: x_train.append(get_vector(x, 100)))\n",
    "total_data_test['Caption'].astype(str).apply(lambda x: x_test.append(get_vector(x, 100)))\n",
    "y_train = utils.to_categorical(total_data_train['Response'].as_matrix(), num_classes=4)\n",
    "y_test = utils.to_categorical(total_data_test['Response'].as_matrix(), num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change X,Y to Numpy Arrays\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to create CNN Model for Image Classification\n",
    "def createModel(size, classes):\n",
    "    model = Sequential()\n",
    "    model.add(keras.layers.LSTM(size, input_shape=(100, 300), return_sequences=False))\n",
    "    model.add(Dense(classes, activation='relu'))  \n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "#Helper function to run model and save intermediate weights\n",
    "def run_model(model, x_train, y_train, x_test, y_test, batch_size, epochs):\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "    filepath=\"KRM_LSTM_weights-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "    check = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max', period = 5)\n",
    "    callbacks_list = [check]\n",
    "    history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, \n",
    "                       validation_data=(x_test, y_test), callbacks=callbacks_list)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create and Fit Model\n",
    "model = createModel(64, 4)\n",
    "model, history = run_model(model, x_train, y_train, x_test, y_test, batch_size=512, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "best_model = load_model('KRM_LSTM_weights-10-0.86.hdf5')\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to create feature maps\n",
    "def get_feature_maps(model, layer_id, input_text):\n",
    "    model_ = Model(inputs=[model.input], outputs=[model.layers[layer_id].output])\n",
    "    return model_.predict(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_array(x, y, label):\n",
    "    final_list = []\n",
    "    count = 0\n",
    "    for ind, i in enumerate(x):\n",
    "        print(ind,\"/\", len(x))\n",
    "        temp_dict = {}\n",
    "        temp_map = list(get_feature_maps(best_model, 0, i.reshape(1,100,300))[0])\n",
    "        temp_dict['Response'] = np.argmax(y[ind])\n",
    "        for ind, j in enumerate(temp_map):\n",
    "            temp_dict[label+str(ind)] = j\n",
    "        final_list.append(temp_dict)\n",
    "    return pd.DataFrame(final_list)\n",
    "\n",
    "df_vec1 = get_array(x_train, y_train, \"Feature_\")\n",
    "df_vec2 = get_array(x_test, y_test, \"Feature_\")\n",
    "main_df = pd.concat([df_vec1, df_vec2])\n",
    "print(df_vec1.shape)\n",
    "print(df_vec2.shape)\n",
    "print(main_df.shape)\n",
    "display(main_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.to_csv('LSTM_DF.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file Text_LSTM.py\n",
    "\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from gensim.models import KeyedVectors\n",
    "import re \n",
    "\n",
    "class Main:\n",
    "    \n",
    "    #Initialize with saved model and embeddings\n",
    "    def __init__(self, path_model, path_embedding):\n",
    "        self.model = load_model(path_model)\n",
    "        self.word_vectors = KeyedVectors.load_word2vec_format(path_embedding, binary=True)\n",
    "        \n",
    "    #Helper function to create feature maps\n",
    "    def get_feature_maps(self, layer_id, input_text):\n",
    "        model_ = Model(inputs=[self.model.input], outputs=[self.model.layers[layer_id].output])\n",
    "        return model_.predict(input_text)\n",
    "    \n",
    "    #Helper function to get embedding\n",
    "    def embedd_text(self, x):\n",
    "        final = []\n",
    "        for i in x:\n",
    "            sequence, count = np.zeros((100, 300), dtype=float), 0\n",
    "            i = re.sub(r'[^\\w\\s]','',i)\n",
    "            for word in i.split():\n",
    "                if word in self.word_vectors.vocab:\n",
    "                    if count<100: \n",
    "                        sequence[count] = self.word_vectors.get_vector(word)\n",
    "                count += 1\n",
    "            final.append(sequence)\n",
    "        return final\n",
    "        \n",
    "    #Helper function to predict \n",
    "    def predict(self, x):\n",
    "        final_list = []\n",
    "        print(\"LSTM Prediction in progress.\\n\")\n",
    "        for ind, i in enumerate(x):\n",
    "            print(ind,\"/\", len(x))\n",
    "            temp_map = list(self.get_feature_maps(0, i.reshape(1,100,300))[0])\n",
    "            final_list.append(temp_map)\n",
    "        return final_list\n",
    "    \n",
    "\n",
    "    #Helper fucntion to combine DF to predictions\n",
    "    def combine(self, df, prediction, label):\n",
    "        new_cols = np.zeros((len(df),len(prediction[0])))\n",
    "        for ind, i in enumerate(prediction):\n",
    "            new_cols[ind,:] = i\n",
    "        for i in range(len(prediction[0])):\n",
    "            df[label+str(i+1)] = new_cols[:,i]\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Text_LSTM\n",
    "\n",
    "#Import Model and Embedding\n",
    "obj = Text_LSTM.Main('KRM_LSTM_New_weights-40-0.87.hdf5', 'google_w2vec.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create DataFrame\n",
    "import pandas as pd\n",
    "df = pd.DataFrame([[\"Hey\",123,\"www.google.co.in\"],[\"Who are you?\",123,\"www.hotmail.co.in\"]], columns=['Post', 'ID','URL'])\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Post\n",
    "x = df['Post']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get embedding for input text\n",
    "x = obj.embedd_text(x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Predictions\n",
    "pred = obj.predict(x)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine Prediction\n",
    "df = obj.combine(df, pred, \"LSTM_Feature_\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
