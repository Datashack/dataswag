{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2 ## issues "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import skimage\n",
    "from skimage import io\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define File-Path to Users Folder\n",
    "#import scikit-image\n",
    "import skimage\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import data\n",
    "from skimage import io\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import glob\n",
    "from skimage.viewer import ImageViewer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "%matplotlib inline\n",
    "\n",
    "#img = io.imread('~/Users/kimia/Desktop/Capstone/imgcluster/Cluster4/11117159_1591691794419786_1496739027_n.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path  = '/Users/kimia/Desktop/Capstone/imgcluster/Cluster4/'\n",
    "dest_img =  '/Users/kimia/Desktop/Capstone/imgcluster/Cluster4/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_imgs = {}\n",
    "img = io.imread(path+'1.jpg')\n",
    "user_imgs[(1, path)] = img\n",
    "img = io.imread(path+'2.jpg')\n",
    "user_imgs[(2, path)] = img\n",
    "#user_imgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images loaded: 2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"num_pic = [0,1,2]\n",
    "def get_images(path):\n",
    "    #Get all Images of Users in the List\n",
    "    user_imgs = {}\n",
    "    for i in num_pic:\n",
    "        path = path + i\n",
    "        for j in glob.glob(path +'.jpg'):\n",
    "            file_name = j.replace(path,'')[1:]\n",
    "            img = io.imread(j)\n",
    "            user_imgs[(i,file_name)] = img\n",
    "    return user_imgs \n",
    "\n",
    "\n",
    "#Get Images from User List and Path\n",
    "user_imgs = get_images(path)\"\"\"\n",
    "print(\"Number of images loaded:\", len(user_imgs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "user_features = []\n",
    "for i in user_imgs.items():\n",
    "    r_mean, r_std, r_med = np.mean(i[1][:,:,0].ravel()), np.std(i[1][:,:,0].ravel()), np.median(i[1][:,:,0].ravel())\n",
    "    g_mean, g_std, g_med  = np.mean(i[1][:,:,1].ravel()), np.std(i[1][:,:,1].ravel()), np.median(i[1][:,:,1].ravel())\n",
    "    b_mean, b_std, b_med  = np.mean(i[1][:,:,2].ravel()), np.std(i[1][:,:,2].ravel()), np.median(i[1][:,:,2].ravel())\n",
    "    canny = np.mean(np.ravel(cv2.Canny(cv2.cvtColor(i[1], cv2.COLOR_BGR2HSV),100,200,L2gradient = True)))\n",
    "    try:\n",
    "        orb = cv2.ORB_create(100)\n",
    "        kp = orb.detect(i[1],None)\n",
    "        kp, des = orb.compute(i[1], kp)\n",
    "        orb_centers = list(KMeans(1).fit([i.pt for i in kp]).cluster_centers_)\n",
    "        orbx1, orby1 = orb_centers[0][0]*255/np.shape(i[1])[0], orb_centers[0][1]*255/np.shape(i[1])[1]\n",
    "    except ValueError:\n",
    "        continue\n",
    "    user_features.append(np.array([i[0][0],i[0][1], r_mean, r_std, r_med, g_mean, g_std, g_med, b_mean, b_std, b_med, canny, orbx1, orby1]))\n",
    "   # return user_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Vectors Created.\n"
     ]
    }
   ],
   "source": [
    "#Helper function to convert image to d-dimension vector\n",
    "def convert(user_imgs):\n",
    "    user_features = []\n",
    "    for i in user_imgs.items():\n",
    "        r_mean, r_std, r_med = np.mean(i[1][:,:,0].ravel()), np.std(i[1][:,:,0].ravel()), np.median(i[1][:,:,0].ravel())\n",
    "        g_mean, g_std, g_med  = np.mean(i[1][:,:,1].ravel()), np.std(i[1][:,:,1].ravel()), np.median(i[1][:,:,1].ravel())\n",
    "        b_mean, b_std, b_med  = np.mean(i[1][:,:,2].ravel()), np.std(i[1][:,:,2].ravel()), np.median(i[1][:,:,2].ravel())\n",
    "        canny = np.mean(np.ravel(cv2.Canny(cv2.cvtColor(i[1], cv2.COLOR_BGR2HSV),100,200,L2gradient = True)))\n",
    "        try:\n",
    "            orb = cv2.ORB_create(100)\n",
    "            kp = orb.detect(i[1],None)\n",
    "            kp, des = orb.compute(i[1], kp)\n",
    "            orb_centers = list(KMeans(1).fit([i.pt for i in kp]).cluster_centers_)\n",
    "            orbx1, orby1 = orb_centers[0][0]*255/np.shape(i[1])[0], orb_centers[0][1]*255/np.shape(i[1])[1]\n",
    "        except ValueError:\n",
    "            continue\n",
    "        user_features.append(np.array([i[0][0],i[0][1], r_mean, r_std, r_med, g_mean, g_std, g_med, b_mean, b_std, b_med, canny, orbx1, orby1]))\n",
    "    return user_features\n",
    "    \n",
    "#Convert Images\n",
    "user_features = convert(user_imgs)\n",
    "print(\"Feature Vectors Created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ClusteringTrain.py\n"
     ]
    }
   ],
   "source": [
    "%%file ClusteringTrain0.py\n",
    "\n",
    "import skimage\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import data\n",
    "from skimage import io\n",
    "import glob\n",
    "from skimage.viewer import ImageViewer\n",
    "import cv2\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "class Clustering:\n",
    "    \n",
    "    #Initialization\n",
    "    def __init__(self, train_path, save_path):\n",
    "        self.train_path = train_path\n",
    "        self.save_path = save_path\n",
    "        \n",
    "    #Helper Function to get images from train path\n",
    "    def get_train_images(self, user_list):\n",
    "        list_users = user_list\n",
    "        self.train_imgs = {}\n",
    "        for i in list_users:\n",
    "            temp_path = self.train_path + i\n",
    "            for j in glob.glob(temp_path + '/*.jpg'):\n",
    "                file_name = j.replace(temp_path,'')[1:]\n",
    "                img = io.imread(j)\n",
    "                self.train_imgs[(i,file_name)] = img \n",
    "        print(\"Number of images loaded:\", len(self.train_imgs))\n",
    "    \n",
    "    #Helper function to convert image to d-dimension vector for each image and \n",
    "    #return dataframe of all images\n",
    "    def convert_to_features(self, columns):\n",
    "        features = []\n",
    "        for i in self.train_imgs.items():\n",
    "            r_mean, r_std, r_med = np.mean(i[1][:,:,0].ravel()), np.std(i[1][:,:,0].ravel()), np.median(i[1][:,:,0].ravel())\n",
    "            g_mean, g_std, g_med  = np.mean(i[1][:,:,1].ravel()), np.std(i[1][:,:,1].ravel()), np.median(i[1][:,:,1].ravel())\n",
    "            b_mean, b_std, b_med  = np.mean(i[1][:,:,2].ravel()), np.std(i[1][:,:,2].ravel()), np.median(i[1][:,:,2].ravel())\n",
    "            canny = np.mean(np.ravel(cv2.Canny(cv2.cvtColor(i[1], cv2.COLOR_BGR2HSV),100,200,L2gradient = True)))\n",
    "            try:\n",
    "                orb = cv2.ORB_create(100)\n",
    "                kp = orb.detect(i[1],None)\n",
    "                kp, des = orb.compute(i[1], kp)\n",
    "                orb_centers = list(KMeans(1).fit([i.pt for i in kp]).cluster_centers_)\n",
    "                orbx1, orby1 = orb_centers[0][0]*255/np.shape(i[1])[0], orb_centers[0][1]*255/np.shape(i[1])[1]\n",
    "            except ValueError:\n",
    "                continue\n",
    "            features.append(np.array([i[0][0],i[0][1], r_mean, r_std, r_med, g_mean, g_std, g_med, b_mean, b_std, b_med, canny, orbx1, orby1]))\n",
    "        df = pd.DataFrame(features, columns = columns)\n",
    "        return df\n",
    "    \n",
    "    def model_images_fit(self, df, k, extra_cols, rand_state):\n",
    "        data = df.copy(deep=True)\n",
    "        \n",
    "        #Delete reference columns\n",
    "        for i in extra_cols:\n",
    "            del data[i]\n",
    "\n",
    "        #Implement Gaussian Mixture Model Algortihm \n",
    "        model = GaussianMixture(n_components=k, random_state=9001)\n",
    "\n",
    "        #Fit Model and Predict\n",
    "        model.fit(data)\n",
    "        y_pred = model.predict_proba(data)\n",
    "\n",
    "        #Add prediction to dataframe and return \n",
    "        for i in range(0,k+1):\n",
    "            if i==k:\n",
    "                label = \"Prediction\"\n",
    "                df[label] = model.predict(data)\n",
    "                break\n",
    "            label = \"Prob_\" + str(i)\n",
    "            df[label] = y_pred[:,i]\n",
    "        return df, model\n",
    "    \n",
    "    def model_users_fit(self, df, k, extra_cols, rand_state):\n",
    "        data = df.copy(deep=True)\n",
    "        \n",
    "        #Delete reference columns\n",
    "        for i in extra_cols:\n",
    "            del data[i]\n",
    "\n",
    "        #Implement K-Means Algortihm\n",
    "        model = KMeans(n_clusters=k, random_state=rand_state)\n",
    "\n",
    "        #Fit Model, Predict and Return\n",
    "        model.fit(data)\n",
    "        y_pred = model.predict(data)\n",
    "        df['Prediction'] = model.labels_\n",
    "        return df, model\n",
    "    \n",
    "    #Helper function to create folders for Image Clustering\n",
    "    def save_clusters(self, df, label):\n",
    "        self.save_path += label + \"/\"\n",
    "        for i in self.train_imgs.items():\n",
    "            temp_row = df[df[\"URL\"]==i[0][1]]\n",
    "            try:\n",
    "                name, pred_folder = i[0][1], str(temp_row['Prediction'].values[0])\n",
    "            except IndexError:\n",
    "                continue\n",
    "            temp_path = self.save_path + \"Cluster\" + pred_folder + \"/\"\n",
    "            if not os.path.exists(temp_path):\n",
    "                os.makedirs(temp_path)\n",
    "            io.imsave(temp_path+name, i[1])\n",
    "        print(\"All Images Saved.\")\n",
    "            \n",
    "    #Helper function to obtain percentage of Cluster Presence\n",
    "    def get_cluster_presence(self, df, k):\n",
    "        cluster_presence = []\n",
    "        for i in list(df['User_Handle'].unique()):\n",
    "            user_dict = {}\n",
    "            temp_df = df[df['User_Handle']==i]\n",
    "            post_count  =  len(temp_df)\n",
    "            user_dict['User_Handle'] = i\n",
    "            for j in range(0, k):\n",
    "                user_dict[\"Cluster_\"+str(j)] = sum(temp_df['Prob_'+str(j)])/post_count\n",
    "            cluster_presence.append(user_dict)\n",
    "        df_presence = pd.DataFrame(cluster_presence)\n",
    "        df_presence = df_presence.fillna(0)\n",
    "        return df_presence\n",
    "    \n",
    "    #Helper function to save model \n",
    "    def save_model(self, model, path):\n",
    "        joblib.dump(model, path) \n",
    "        print(\"Model Saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ClusteringTest.py\n"
     ]
    }
   ],
   "source": [
    "%%file ClusteringTest0.py\n",
    "\n",
    "import skimage\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import data\n",
    "from skimage import io\n",
    "import glob\n",
    "from skimage.viewer import ImageViewer\n",
    "import cv2\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "class Ranking:\n",
    "    \n",
    "    #Initialization\n",
    "    def __init__(self, target_path):\n",
    "        self.target_path = target_path\n",
    "\n",
    "    #Helper Function to get images from target path \n",
    "    def get_images_target(self):\n",
    "        #Get all Images of Users in the List\n",
    "        self.target_imgs = {}\n",
    "        for j in glob.glob(self.target_path + '/*.jpg'):\n",
    "            file_name = j.replace(self.target_path,'')[:]\n",
    "            img = io.imread(j)\n",
    "            self.target_imgs[(\"Input/Target\",file_name)] = img\n",
    "        print(\"Number of images loaded:\", len(self.target_imgs))\n",
    "    \n",
    "    #Helper function to load saved model\n",
    "    def load_model(self, path):\n",
    "        model = joblib.load(path)\n",
    "        print (\"Model loaded.\")\n",
    "        return model\n",
    "\n",
    "    #Helper function to convert image to d-dimension vector for each image and \n",
    "    #return dataframe of all images\n",
    "    def convert_to_features(self, columns):\n",
    "        features = []\n",
    "        for i in self.target_imgs.items():\n",
    "            r_mean, r_std, r_med = np.mean(i[1][:,:,0].ravel()), np.std(i[1][:,:,0].ravel()), np.median(i[1][:,:,0].ravel())\n",
    "            g_mean, g_std, g_med  = np.mean(i[1][:,:,1].ravel()), np.std(i[1][:,:,1].ravel()), np.median(i[1][:,:,1].ravel())\n",
    "            b_mean, b_std, b_med  = np.mean(i[1][:,:,2].ravel()), np.std(i[1][:,:,2].ravel()), np.median(i[1][:,:,2].ravel())\n",
    "            canny = np.mean(np.ravel(cv2.Canny(cv2.cvtColor(i[1], cv2.COLOR_BGR2HSV),100,200,L2gradient = True)))\n",
    "            try:\n",
    "                orb = cv2.ORB_create(100)\n",
    "                kp = orb.detect(i[1],None)\n",
    "                kp, des = orb.compute(i[1], kp)\n",
    "                orb_centers = list(KMeans(1).fit([i.pt for i in kp]).cluster_centers_)\n",
    "                orbx1, orby1 = orb_centers[0][0]*255/np.shape(i[1])[0], orb_centers[0][1]*255/np.shape(i[1])[1]\n",
    "            except ValueError:\n",
    "                continue\n",
    "            features.append(np.array([i[0][0],i[0][1], r_mean, r_std, r_med, g_mean, g_std, g_med, b_mean, b_std, b_med, canny, orbx1, orby1]))\n",
    "        df = pd.DataFrame(features, columns = columns)\n",
    "        return df\n",
    "\n",
    "    #Helper function to make prediction for target images using image model\n",
    "    def predict(self, df, model, k, cluster_names, extra_cols):\n",
    "        data = df.copy(deep=True)\n",
    "        \n",
    "        #Delete reference columns\n",
    "        for i in extra_cols:\n",
    "            del data[i]\n",
    "            \n",
    "        #Make Prediction\n",
    "        y_pred = model.predict_proba(data)\n",
    "        \n",
    "        #Add prediction to dataframe and return \n",
    "        for i in range(0,k+1):\n",
    "            if i==k:\n",
    "                label = \"Prediction\"\n",
    "                df[label] = model.predict(data)\n",
    "                break\n",
    "            label = cluster_names[i] + \" (\" + str(i) + \")\"\n",
    "            df[label] = y_pred[:,i]\n",
    "        return df\n",
    "    \n",
    "    #Helper function to generate distance dictionary\n",
    "    def get_result(self, df, k, model):\n",
    "        final_dict = {}\n",
    "        for i in range(0,len(df)):\n",
    "            temp_file = df.iloc[i,1]\n",
    "            temp_dist = []\n",
    "            for j in range(0, k):\n",
    "                temp_dist.append(np.linalg.norm(df.iloc[i,14:14+k].astype(float)-model.cluster_centers_[j]))\n",
    "            final_dict[temp_file] = temp_dist\n",
    "        return final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ClusteringTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model_kmeans.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-03a41489d0f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_kmeans.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_read_fileobject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model_kmeans.pkl'"
     ]
    }
   ],
   "source": [
    "joblib.load('model_kmeans.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/kimia/Desktop/Capstone/hotel/Image Analysis/model_kmeans.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-455aee1fc7e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_kmeans_users\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/kimia/Desktop/Capstone/hotel/Image Analysis/model_kmeans.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_read_fileobject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/kimia/Desktop/Capstone/hotel/Image Analysis/model_kmeans.pkl'"
     ]
    }
   ],
   "source": [
    "model_kmeans_users = joblib.load('/Users/kimia/Desktop/Capstone/hotel/Image Analysis/model_kmeans.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/kimia/Desktop/Capstone/hotel/Image Analysis/model_kmeans.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-d318df36f574>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlocation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/kimia/Desktop/Capstone/hotel/Image Analysis/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfullpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_kmeans.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfullpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_read_fileobject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/kimia/Desktop/Capstone/hotel/Image Analysis/model_kmeans.pkl'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "location = '/Users/kimia/Desktop/Capstone/hotel/Image Analysis/'\n",
    "fullpath = os.path.join(location, 'model_kmeans.pkl')\n",
    "model = joblib.load(fullpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
